{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise on the value of unsupervised constructed features for training a classifier with few labeled examples\n",
    "\n",
    "To get unsupervised constructed features of an image, we have used a pretrained CNN as feature extractor. For this purpose we pushed each image through a pretrained CNN and extracted the activations in the first fully connected layer. As pretrained CNN we use a VGG16 architecture that was trained on ImageNet data and was the second winner of the ImageNet competition in 2014.\n",
    "\n",
    "In this manner we have got unsupervised constructed features for 1000 images of the MNIST data set and 1000 images of the CIFAR10 data set. In both data sets we have 10 distinguished classes. The data sets are balanced meaning we have 100 images per class. \n",
    "\n",
    "In the last exercise we have used 2D plots to check if the feature quality is good enough to lead to visible clusters corresoonding to the 10 classes in the data. For the MNISt data it is very easy to seperate the 10 digits. Therefore we will only continue with the more challenging CIFAR10 data.\n",
    "\n",
    "As a second check on the quality of the feature representation of the CIFAR10 data, we will use once the pixel-features and once the VGG-features to train a classifier using only 100 labeled data (on average 10 per class). If the VGG-feature are indeed better, we would expect to achieve a better classifier when using the VGG-feature compared to the pixel feature.\n",
    "\n",
    "a) Which accuracy would you expect for a classifier which cannot distinguish between the 10 classes and is only guessing?\n",
    "\n",
    "\n",
    "b) Go through the code which is used to set-up, train, and evaluate a CNN classifier using the raw pixel features. Discuss your thoughts on the achieved accuracy (e.g. with your neighbor).\n",
    "\n",
    "\n",
    "b) Now we use the unsupervised constructed VGG features. We want to check, if these VGG features are good enough to train a classifier with only few labeled data and still get a satisfying performance. For this purpose, please complet the code to set up a fully connected NN and run the provided subsequent code to train it and determine its accuracy on the test set. Compare it to the accuracy which we achieve with a RF. Discuss the results (e.g. with your neighbor).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as imgplot\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from pylab import *\n",
    "\n",
    "\n",
    "import time\n",
    "import tensorflow as tf\n",
    "tf.set_random_seed(1)\n",
    "\n",
    "import keras\n",
    "import sys\n",
    "print (\"Keras {} TF {} Python {}\".format(keras.__version__, tf.__version__, sys.version_info))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CIFAR Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#downlad cifar data\n",
    "from keras.datasets import cifar10\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "del [x_test,y_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loop over each class label and sample 100 random images over each label and save the idx to subset\n",
    "np.random.seed(seed=222)\n",
    "idx=np.empty(0,dtype=\"int8\")\n",
    "for i in range(0,len(np.unique(y_train))):\n",
    "    idx=np.append(idx,np.random.choice(np.where((y_train[0:len(y_train)])==i)[0],100,replace=False))\n",
    "\n",
    "x_train= x_train[idx]\n",
    "y_train= y_train[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(np.unique(y_train,return_counts=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make train vaild and test\n",
    "#loop over each class label and sample 100 random images over each label and save the idx to subset\n",
    "np.random.seed(seed=123)\n",
    "idx_train=np.empty(0,dtype=\"int8\")\n",
    "for i in range(0,len(np.unique(y_train))):\n",
    "    idx_train=np.append(idx_train,np.random.choice(np.where((y_train[0:len(y_train)])==i)[0],10,replace=False))\n",
    "\n",
    "x_train_new = x_train[idx_train]\n",
    "y_train_new = y_train[idx_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_new=(np.delete(x_train,idx_train,axis=0))\n",
    "y_test_new=(np.delete(y_train,idx_train,axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(seed=127)\n",
    "idx_vaild=np.empty(0,dtype=\"int8\")\n",
    "for i in range(0,len(np.unique(y_test_new))):\n",
    "    idx_vaild=np.append(idx_vaild,np.random.choice(np.where((y_test_new[0:len(y_test_new)])==i)[0],10,replace=False))\n",
    "\n",
    "x_vaild_new = x_test_new[idx_vaild]\n",
    "y_valid_new = y_test_new[idx_vaild]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_new=(np.delete(x_test_new,idx_vaild,axis=0))\n",
    "y_test_new=(np.delete(y_test_new,idx_vaild,axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_new = np.reshape(x_train_new, (100,32,32,3))\n",
    "x_vaild_new = np.reshape(x_vaild_new, (100,32,32,3))\n",
    "x_test_new = np.reshape(x_test_new, (800,32,32,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.unique(y_train_new,return_counts=True))\n",
    "print(np.unique(y_valid_new,return_counts=True))\n",
    "print(np.unique(y_test_new,return_counts=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils.np_utils import to_categorical   \n",
    "\n",
    "y_train_new=to_categorical(y_train_new,10)\n",
    "y_valid_new=to_categorical(y_valid_new,10)\n",
    "y_test_new=to_categorical(y_test_new,10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_train_new.shape)\n",
    "print(y_train_new.shape)\n",
    "\n",
    "print(x_vaild_new.shape)\n",
    "print(y_valid_new.shape)\n",
    "\n",
    "print(x_test_new.shape)\n",
    "print(y_test_new.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# center and standardize the data\n",
    "X_mean = np.mean( x_train_new, axis = 0)\n",
    "X_std = np.std( x_train_new, axis = 0)\n",
    "\n",
    "x_train_new = (x_train_new - X_mean ) / (X_std + 0.0001)\n",
    "x_vaild_new = (x_vaild_new - X_mean ) / (X_std + 0.0001)\n",
    "x_test_new = (x_test_new - X_mean ) / (X_std + 0.0001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up the the CNN classifier based on raw image data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout, BatchNormalization\n",
    "from keras.layers import Convolution2D, MaxPooling2D, Flatten\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we define  hyperparameter of the NN\n",
    "batch_size = 10\n",
    "nb_classes = 10\n",
    "nb_epoch = 30\n",
    "img_rows, img_cols = 32, 32\n",
    "kernel_size = (3, 3)\n",
    "input_shape = (img_rows, img_cols, 3)\n",
    "pool_size = (2, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Convolution2D(8,kernel_size,padding='same',input_shape=input_shape))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Convolution2D(8, kernel_size,padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=pool_size))\n",
    "\n",
    "model.add(Convolution2D(16, kernel_size,padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Convolution2D(16,kernel_size,padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=pool_size))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(40))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(nb_classes))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history=model.fit(x_train_new, y_train_new, \n",
    "                  batch_size=10, \n",
    "                  epochs=30,\n",
    "                  verbose=2, \n",
    "                  validation_data=(x_vaild_new, y_valid_new),shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation of the CNN classifier that was trained on raw image data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "pred=model.predict(x_test_new)\n",
    "print(confusion_matrix(np.argmax(y_test_new,axis=1),np.argmax(pred,axis=1)))\n",
    "print(\"Acc = \" ,np.sum(np.argmax(y_test_new,axis=1)==np.argmax(pred,axis=1))/len(y_test_new))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting the VGG features for CIFAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downloading embeddings\n",
    "import urllib\n",
    "import os\n",
    "if not os.path.isfile('cifar_EMB_1000.npz'):\n",
    "    urllib.request.urlretrieve(\n",
    "    \"https://www.dropbox.com/s/si287al91c1ls0d/cifar_EMB_1000.npz?dl=1\",\n",
    "    \"cifar_EMB_1000.npz\")\n",
    "%ls -hl cifar_EMB_1000.npz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data=np.load(\"cifar_EMB_1000.npz\")\n",
    "vgg_features_cifar = Data[\"arr_0\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg_features_cifar_train = vgg_features_cifar[idx_train]\n",
    "vgg_features_cifar_test=(np.delete(vgg_features_cifar,idx_train,axis=0))\n",
    "vgg_features_cifar_valid = vgg_features_cifar_test[idx_vaild]\n",
    "vgg_features_cifar_test=(np.delete(vgg_features_cifar_test,idx_vaild,axis=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vgg_features_cifar_train.shape)\n",
    "print(vgg_features_cifar_valid.shape)\n",
    "print(vgg_features_cifar_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up the the CNN classifier based on VGG feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(200,batch_input_shape=(None, 4096)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(200))\n",
    "\n",
    "#### we still need to add the last layers to get the predictions on the 10 classes\n",
    "### your code here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "####### end of your code ######\n",
    "\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history=model.fit(vgg_features_cifar_train, y_train_new, \n",
    "                  batch_size=10, \n",
    "                  epochs=20,\n",
    "                  verbose=2, \n",
    "                  validation_data=(vgg_features_cifar_valid, y_valid_new),shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation of the CNN classifier that was trained on VGG features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred=model.predict(vgg_features_cifar_test)\n",
    "\n",
    "#### we now want to get the confusion matrix for the predictions on the test data\n",
    "### your code here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########## end of your code ###############################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline: use VGG feature to train a Random Forest model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf = RandomForestClassifier()\n",
    "clf.fit(vgg_features_cifar_train,np.argmax(y_train_new, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "pred=clf.predict(vgg_features_cifar_test)\n",
    "print(confusion_matrix(np.argmax(y_test_new, axis=1), pred))\n",
    "np.sum(pred==np.argmax(y_test_new, axis=1))/len(np.argmax(y_test_new, axis=1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
